---
title: An Exponential Efron-Stein Inequality for Lq Stable Learning Rules
abstract: "There is an accumulating evidence in the literature that \\emph{stability
  of learning algorithms} is a key characteristic that permits a learning algorithm
  to generalize. Despite various insightful results in this direction, there seems
  to be an overlooked dichotomy in the type of stability-based generalization bounds
  we have in the literature. On one hand, the literature seems to suggest that exponential
  generalization bounds for the estimated risk, which are optimal, can be \\emph{only}
  obtained through \\emph{stringent}, \r \\emph{distribution independent} and \\emph{computationally
  intractable} notions of stability such as \\emph{uniform stability}. On the other
  hand, it seems that \\emph{weaker} notions of stability such as hypothesis stability,
  although it is \\emph{distribution dependent} and more \\emph{amenable} to computation,
  can \\emph{only} yield polynomial generalization bounds for the estimated risk,
  which are suboptimal.\r In this paper, we address the gap between these two regimes
  of results. In particular, the main question we address here is \\emph{whether it
  is possible to derive exponential generalization bounds for the estimated risk using
  a notion of stability that is computationally tractable and distribution dependent,
  but weaker than uniform stability}.\r Using recent advances in concentration inequalities,
  and using a notion of stability that is weaker than uniform stability but distribution
  dependent and amenable to computation, we derive an exponential tail bound for the
  concentration of the estimated risk of a hypothesis returned by a \\emph{general}
  learning rule, where the estimated risk is expressed in terms of either the resubstitution
  estimate (empirical error), or the deleted (or, leave-one-out) estimate. As an illustration
  we derive exponential tail bounds for ridge regression with \\emph{unbounded responses}
  – a setting where uniform stability results of Bousquet and Elisseeff (2002) are
  not applicable."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: abou-moustafa19a
month: 0
tex_title: An Exponential Efron-Stein Inequality for Lq Stable Learning Rules
firstpage: 31
lastpage: 63
page: 31-63
order: 31
cycles: false
bibtex_author: Abou-Moustafa, Karim and Szepesv\'ari, Csaba
author:
- given: Karim
  family: Abou-Moustafa
- given: Csaba
  family: Szepesvári
date: 2019-03-10
address: 
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Algorithmic Learning
  Theory
volume: '98'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 3
  - 10
pdf: http://proceedings.mlr.press/v98/abou-moustafa19a/abou-moustafa19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
